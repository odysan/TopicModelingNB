{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Not Sorted LDA Cluster](https://raw.githubusercontent.com/odysan/TopicModelingNB/master/LDANotSorted.png)\n",
    "\n",
    "![Sorted LDA Cluster](https://raw.githubusercontent.com/odysan/TopicModelingNB/master/LDASorted.png)\n",
    "\n",
    "## Background Information\n",
    "\n",
    "The main ideas behind Latent Dirichlet Allocation is that:\n",
    "* Every document consists of different topics\n",
    "* Some documents consist of a larger proportion of one topic compared to another\n",
    "* Each topic has certain words that are more reflective of that topic\n",
    "\n",
    "For example, if we had topic \"Dog\", we could say that the words \"bark\" and \"fetch\" would be more reflective of that topic.  On the other hand, if we had topic \"Cat\", we could say that the words \"meow\" and \"litter\" would be more reflective of the topic \"Cat\".\n",
    "\n",
    "This project uses LDA to cluster abstracts of the National Science Foundation abstracts corpus, and visualizes a small sample of them.  The documents are available from https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/.\n",
    "\n",
    "## Python Code\n",
    "\n",
    "The code in this Jupyter Notebook is available here: https://github.com/odysan/TopicModel\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## Required Python Modules\n",
    "\n",
    "We will need to import the following Python modules in order to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import walk # traversing the data\n",
    "from nltk.corpus import stopwords # removing stopwords\n",
    "from nltk.stem.porter import PorterStemmer # stemming tokens\n",
    "from nltk import word_tokenize # tokenizing each sentence\n",
    "import string # removing punctuation\n",
    "from gensim import corpora, models # for LDA\n",
    "import numpy as np # for visualization\n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import multiprocessing as mp # for multithreading file reads\n",
    "from collections import defaultdict # for organizing topic distributions into buckets\n",
    "import optparse # for command line arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing A Research Paper from the National Science Foundation\n",
    "\n",
    "First, download the archives from https://archive.ics.uci.edu/ml/machine-learning-databases/nsfabs-mld/.  At the time of writing, part 1, part 2, and part 3 were available.\n",
    "\n",
    "Scanning a few documents shows that they have document metadata at the top of the file, while the bottom is reserved for the actual abstract -- where it is explicitly delimited \"Abstract :\".  As well, each abstract is limitted to a certain number of words per line so that the actual abstract is separated into several lines.  We can easily parse through the file by looking for the line that matches the \"Abstract :\" delimiter.\n",
    "\n",
    "When it comes to Natural Language Processing, we need to clean the sentence that we will be performing analysis on.  This involves removing punctuation, tokenizing, removing stop words, and stemming each word.\n",
    "\n",
    "If we come across a character that is not parseable, or an error is raised while parsing, we ignore that abstract altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file(filepath):\n",
    "    \"\"\"Reads the NSF paper and returns the abstract.  Each abstract is tokenized, stripped of its stop words, and\n",
    "    stemmed.\n",
    "\n",
    "    Returns string\n",
    "    \"\"\"\n",
    "    found_abstract = False\n",
    "    processed_line = []\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            if \"Abstract    :\" in line:\n",
    "                found_abstract = True\n",
    "                continue\n",
    "            if found_abstract:\n",
    "                try:\n",
    "                    line = line.lower().translate(None, string.punctuation)\n",
    "                    splitted_line = word_tokenize(line)\n",
    "                    removed_stop_words = [word for word in splitted_line if word not in stopwords.words('english')]\n",
    "                    stemmed_words = [stemmer.stem(i) for i in removed_stop_words]\n",
    "                    processed_line.extend(stemmed_words)\n",
    "                except:\n",
    "                    return -1\n",
    "    return processed_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is the callback for the above function.  It is used to analyze the output of the multithreaded child process.  We discard any erroneous data, and ignore any abstract that contains less than 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_file_callback(return_val):\n",
    "    \"\"\"Handles the return value of each child process.  Ignores all abstracts that failed parsing and do not contain\n",
    "    more than 10 cleaned tokens.\n",
    "    \"\"\"\n",
    "    if return_val != -1 and len(return_val) > 10:\n",
    "        all_sentences.append(return_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA On The Abstracts\n",
    "\n",
    "Since the gensim library takes care of the LDA algorithm, we won't define a new function just for LDA.  So from this point on, we already ran the LDA algorithm on all of the valid (e.g. non-erroneous and len(abstract) > 10)) abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Documents By Topic (for Visualization)\n",
    "\n",
    "Visualization is a big part of analytical roles.  We want to be able to see how effectively the algorithm can cluster several different datasets.  Because of this, we need to organize a small sample of our documents so that we can easily see which topic they belong to.\n",
    "\n",
    "In order to do that, we need to organize them by buckets (which is based on each topic).  Numerical order does not matter so much in this case, since each topic is essentially a categorical value.  We will do this by building a dictionary of topic id:list of documents that mostly reflect that topic id.  ie. documents that talk about dogs will hash to topic \"dog\", documents that talk about cats will has to topic about \"cat\".\n",
    "\n",
    "The first step is to find the dominant topic in a given document.  We then direct that document to its corresponding dominant topic in the dictionary as mentioned above.\n",
    "\n",
    "We continue to do this until all topics have greater than a pre-specified amount of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hash_topics_to_buckets(corp, lda_model, num_of_topics, min_docs):\n",
    "    \"\"\"Takes a non-random sample of all of the NSF papers and returns them for the sake of visualization.  Stops when\n",
    "    there are at least min_docs (int) documents in each topic.\n",
    "\n",
    "    Returns a dictionary of (k, v) = (topic id, tuple(document id, document's topic distribution))\"\"\"\n",
    "    topics_hash = defaultdict(list)\n",
    "    vis_topics = []\n",
    "    arbitrary_id = 0\n",
    "    for doc_vec in corp:\n",
    "        max_topic_id = -9999\n",
    "        max_topic_dist = -9999\n",
    "        topic_probs = [0] * num_of_topics\n",
    "        for (topic_id, topic_dist) in lda_model.get_document_topics(doc_vec, minimum_probability=0.005):\n",
    "            if topic_dist > max_topic_dist:\n",
    "                max_topic_dist = topic_dist\n",
    "                max_topic_id = topic_id\n",
    "            topic_probs[topic_id] = topic_dist\n",
    "        arbitrary_id += 1\n",
    "        topics_hash[max_topic_id].append((arbitrary_id, topic_probs))\n",
    "        vis_topics.append((arbitrary_id, topic_probs))\n",
    "        finished = True\n",
    "        for x in range(0, num_of_topics):\n",
    "            if len(topics_hash[x]) < min_docs:\n",
    "                finished = False\n",
    "        if finished:\n",
    "            # (ids, distributions_matrix) = create_vis_data(vis_topics, num_of_topics)\n",
    "            # visualize(ids, distributions_matrix)\n",
    "            break\n",
    "    return topics_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Visualization-friendly Data\n",
    "\n",
    "The matplotlib documentation uses numpy arrays a lot, so we will be inserting our data from the dictionary to one numpy array.  Our function will also work with lists, but it won't be organized like how our topic buckets are organized.\n",
    "\n",
    "The function cycles through each key (which is sorted) and inserts the distribution data into a matrix.  As well, it inserts the document ID into another list for the sake of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vis_data(collection, num_of_topics):\n",
    "    \"\"\"Based on a dictionary or a list, creates a distribution matrix (using numpy) and list of id's associated with\n",
    "    each record for the sake of visualization.\n",
    "\n",
    "    Returns tuple(list of id's, dist matrix)\"\"\"\n",
    "    list_of_ids = []\n",
    "    dist_matrix = np.vstack([[0] * num_of_topics])\n",
    "    if isinstance(collection, defaultdict):\n",
    "        for key in sorted(collection):\n",
    "            for value in collection[key]:\n",
    "                dist_matrix = np.vstack([dist_matrix, value[1]])\n",
    "                list_of_ids.append(value[0])\n",
    "    elif isinstance(collection, list):\n",
    "        for tup in collection:\n",
    "            dist_matrix = np.vstack([dist_matrix, tup[1]])\n",
    "            list_of_ids.append(tup[0])\n",
    "    dist_matrix = np.delete(dist_matrix, (0), axis=0)\n",
    "    return list_of_ids, dist_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now, with the list of document IDs and the distributions matrix, we can visualize the data.  It sets the document IDs as the x-tick labels, and the topic ID as the y-tick labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize(ids, dists):\n",
    "    \"\"\"Visualizes distributions matrix.\"\"\"\n",
    "    weights_graph(dists)\n",
    "    (num_cols, num_rows) = dists.shape\n",
    "    plt.yticks(range(0, num_rows), range(0, num_rows))\n",
    "    plt.ylabel('Topic ID')\n",
    "    plt.xticks(range(0, len(ids)), ids)\n",
    "    plt.xlabel('Document ID')\n",
    "    plt.title('Document distributions per topic')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing\n",
    "\n",
    "The data is visualized in a graph.  Each row is a ropic, and each column is a document.  Darker squares mean that the document represents that topic very well.  Lighter squares mean the opposite.  This code is heavily based on the hinton diagram as seen on the matplotlib documentation via http://matplotlib.org/examples/specialty_plots/hinton_demo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_graph(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw diagram for visualizing a weight matrix.  Y-axis represents topic ID, X-axis represents document ID.\n",
    "    Darker squares imply that document represents that topic more than it represents other topics.  The lighter the\n",
    "    square, the lower its representation of that topic.\n",
    "    ie. each row is a topic, each column is a document, and darker squares mean that the document represents that\n",
    "    topic very well.\n",
    "\n",
    "    Heavily based on the hinton diagram here: http://matplotlib.org/examples/specialty_plots/hinton_demo.html\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    ax.patch.set_facecolor('lightgray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = (0,0,0, w)\n",
    "        size = 0.9\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "By accepting command-line arguments, we can change the data directory that we contain the folders in.  As well, we can change the number of topics that we want to cluster for, specify a different number of iterations of LDA, change the number of workers we use when multithreading processes, and choose a different minimum number of documents per topic when visualizing.\n",
    "\n",
    "To start, we multithread the function which reads files.  By using an SSD it cuts processing time down to a fraction of what it used to, since we are no longer held back by seek time.\n",
    "\n",
    "After all abstracts have been read and processed, we prepare a dictionary of all of the different tokens.\n",
    "\n",
    "We then remove all of the extreme tokens.  Each token that appears in less than 5 documents is filtered out.  Each token that appears in more than half of the documents is filtered out.  Then we create a bag-of-words list for each document.\n",
    "\n",
    "Next, we run the multithreaded LDA algorithm on the processed abstracts.\n",
    "\n",
    "Finally, we visualize the results.  The results can be seen from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accept command-line arguments\n",
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-d\", \"--data\", dest='data', default=\"./data\", type=str,\n",
    "                     help=\"The directory containing the NSF papers that we will be clustering. (default: ./)\")\n",
    "optparser.add_option(\"-t\", \"--numtopics\", dest='num_of_topics', default=5, type=int,\n",
    "                     help=\"Number of topics we want to group our papers into. (default: 5)\")\n",
    "optparser.add_option(\"-i\", \"--iterlda\", dest='iterations_lda', default=20, type=int,\n",
    "                     help=\"Number of times the LDA algorithm should iterate for topic modeling. (default: 20\")\n",
    "optparser.add_option(\"-w\", \"--numworkers\", dest='num_of_workers', default=mp.cpu_count(), type=int,\n",
    "                     help=\"Number of workers for multithreading (default: max available for CPU)\")\n",
    "optparser.add_option(\"-k\", \"--mindocs\", dest='min_docs', default=5, type=int,\n",
    "                     help=\"Visualize a sample of all documents that reflect at least k of each topic. \"\n",
    "                          \"(default: 5)\")\n",
    "(opts, _) = optparser.parse_args()\n",
    "\n",
    "# Multithreaded read files\n",
    "pool = mp.Pool(opts.num_of_workers)\n",
    "stemmer = PorterStemmer()\n",
    "all_sentences = []\n",
    "all_words = set()\n",
    "for (dirpath, dirnames, filenames) in walk(opts.data):\n",
    "    if len(dirnames) == 0:\n",
    "        for filename in filenames:\n",
    "            if filename not in [\"links.html\", \"index.html\"]:\n",
    "                pool.apply_async(process_file, args=(dirpath + \"/\" + filename, ), callback=process_file_callback)\n",
    "pool.close()\n",
    "pool.join()\n",
    "print \"Finished reading file.\"\n",
    "\n",
    "# Prepare dictionary of all of the different tokens\n",
    "dictionary = corpora.Dictionary(all_sentences)\n",
    "print \"Finished preparing dictionary.\"\n",
    "\n",
    "# Remove noisy tokens and create bag-of-words list\n",
    "dictionary.filter_extremes()\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in all_sentences]\n",
    "print \"Finished preparing corpus.\"\n",
    "\n",
    "# Run multithreaded LDA on processed abstracts\n",
    "ldamodel = models.LdaMulticore(corpus=corpus, num_topics=opts.num_of_topics, id2word=dictionary,\n",
    "                               passes=opts.iterations_lda, workers=opts.num_of_workers)\n",
    "print \"Finished building LDA model.\"\n",
    "\n",
    "# Prepare buckets for visualization\n",
    "topics_hash = hash_topics_to_buckets(corpus, ldamodel, opts.num_of_topics, opts.min_docs)\n",
    "print \"Finished sorting by topic.\"\n",
    "\n",
    "# Visualize!\n",
    "(ids, distributions_matrix) = create_vis_data(topics_hash, opts.num_of_topics)\n",
    "visualize(ids, distributions_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
